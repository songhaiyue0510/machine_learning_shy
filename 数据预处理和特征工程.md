[TOC]

# 1、概述

## 1.1 数据预处理与特征工程

| 数据挖掘的五大流程                                           |
| ------------------------------------------------------------ |
| 1、获取数据<br>**2、数据预处理<br>**（1）从数据中检测、纠正或者删除损坏、不准确、不适用模型的记录的过程<br>（2）可能面对的问题：<br>          *  数据类型不同，文字、数字、含时间序列、连续、离散等<br>          *  数据质量不行，噪声、异常、缺失、出错、量纲不一、数据是偏态、数据量太大或太小<br>（3）目的：让数据适应模型，匹配模型的需求<br>**3、特征工程**<br>（1）原始数据→更能代表预测模型的潜在问题的特征的过程，可以通过挑选最相关的特征、提取特征、创造特征（降维算法）实现<br>（2）可能面对的问题：特征之间有相关性、特征和标签无关、特征太多或太少、特征无法表现出应有的数据现象或真实面貌<br>（3）目的：降低计算成本，提升模型上限<br>4、建模，测试模型并预测出结果<br>5、上线，验证模型结果 |

## 1.2 sklearn 中的数据处理和特征工程

<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200331093449173.png" alt="image-20200331093449173" style="zoom:50%;" />

- 模块Preocessing：几乎包含预处理的所有内容
- 模块Impute: 填补缺失值专用
- 模块feature_selection：包含特征选择的各种方法的时间
- 模块decomposition：包含降维算法

# 2、数据预处理Processing & Impute

## 2.1 无量纲化

无量纲化：即不同规格的数据转换到统一规格，或不同分布的数据转换到某个特定分布

- 以梯度和矩阵为核心的算法中，如逻辑回归、支持向量机、神经网络等，无量纲化可以**加快求解速度**
- 在距离类模型中，如KNN、K-Means中，无量纲化可以**提升模型精度**
- 决策树和树的集成算法们，不需要无量纲化

$$
无量纲化
\begin{cases}
线性无量纲化
\begin{cases}
中心化（zero-centerd、mean-subtraction):所有记录减去一个值，即将所有数据评议到某个位置\\
缩放处理（scale)：所有记录除以一个固定值，将数据固定在一个范围之中，取对数也是一种缩放处理
\end{cases}\\
非线性无量纲化
\end{cases}
$$

<font color=#0000FF size=2 face="黑体">***常用的两种：***</font>

**数据归一化**（Preprocessing.MinMaxScaler）:  $x^*=\frac {x-min(x)}{max(x)-min(x)}$，归一化后数据服从正态分布，默认压缩到[0,1]，易受异常值影响

**数据标准化**（preprocessing.standardScaler)：$x^*=\frac {x-\mu}{\sigma}$，标准化后数据服从标准正态分布（均值0、方差1）

* 空值NaN会被当做缺失值，在fit时忽略，在transform时保持NaN的状态显示
* 特征矩阵X至少为2维矩阵
* 通常选择StandardScaler进行特征缩放（效果不好换MinMaxScaler），在PCA、聚类、逻辑回归、神经网络等优先选择
* 在不涉及距离度量、梯度、协方差计算以及数据需要被压缩到特定区间时，使用MinMaxScaler

<font color=#0000FF size=2 face="黑体">***无量纲化手段：：***</font>

| <font size=1>无量纲化</font> | <font size=1>功能</font> | <font size=1>中心化</font> | <font size=1>缩放</font> | 详解                                                         |
| ------| --------- | ---------- | ------------ | ------------------------------------------------------------ |
| <font size=0.8>.StandardScaler</font> | <font size=0.5>标准化</font> | <font size=0.5>均值</font> | <font size=0.5>方差</font> | <font size=0.5>通过减掉均值并缩放到单位方差来标准化特征，标准化后服从标准正态分布（方差1均值0）</font> |
| <font size=0.8>.MinMasScaler</font> | <font size=0.5>归一化</font> | <font size=0.5>最小值</font> | <font size=0.5>极差</font> | <font size=0.5>通过最大值最小值将每个特征缩放到给定范围，默认[0,1]</font> |
| <font size=0.8>.MaxAbsScaler</font> | <font size=0.5>缩放</font> | <font size=0.5>N/A</font>  | <font size=0.5>最大值</font> | <font size=0.5>通过除以特征中**绝对值最大的数值的绝对值**，将数据压缩到[-1,1]之间，无中心化，<br>**不会破坏数据的稀疏性**，即对数据中取值为0的数据不影响 </font> |
| <font size=0.8>.RobustScaler</font> | <font size=0.5>无量纲化</font> | <font size=0.5>中位数</font> | <font size=0.5>四分位数范围</font> | <font size=0.5>可以处理异常值，对异常值不敏感的统计量来缩放数据<br>这个缩放器减去中位数并除以百分位数范围（IQR: Interquartile Range）缩放数据，IQR为第一分位数（25%）和第三分位数（75%）之间的范围<br>**当异常值很多、噪音很大时，此方法效果较好**</font> |
| <font size=0.8>.Normalizer</font> | <font size=0.5>无量纲化</font> | <font size=0.5>N/A</font> | <font size=0.5>sklearn中未明确，依范数原理：<br>l1:样本向量的长度/样本中每个元素的绝对值的和<br>l2: 样本向量的长度/样本中每个元素的欧氏距离</font> | <font size=0.5>将样本缩放到单位范数<br>将输入的数据缩放到单位范数是文本分类或者聚类的常见操作<br>使用参数norm来确定正则化的番薯方向，可以选择”l1"、"l2"、"max"三种选项，默认l2范数<br>这个评估器的fit接口什么也不做，但在管道中依然有用</font> |
| <font size=0.8>.PowerTransformer</font> | <font size=0.5>非线性无量纲化</font> | <font size=0.5>N/A</font> | <font size=0.5>N/A</font> | <font size=0.5>应用特征功率变换使数据更接近正态分布<br>功率变换使一系列参数单调变换，使数据更像高斯，对于建模与异方差性、活其他需要正态性的情况非常有用<br>要求数据严格为正，power_transform()通过最大似然估计来稳定方差并确定最小化偏度的最佳参数<br>默认情况下，标准化应用于转换后的数据</font> |
| <font size=0.8>.QuantileTransformer</font> | <font size=0.5>非线性无量纲化</font> | <font size=0.5>N/A</font> | <font size=0.5>N/A</font> | <font size=0.5>使用百分位转换特征，通过缩小边缘异常值和非异常值之间的距离来提供特征的非线性变换，可以使用参数output_distribution="normal"来将数据映射到标准正态分布</font> |
| <font size=0.8>.KernelCenterer</font> | <font size=0.5>中心化</font> | <font size=0.5>均值</font> | <font size=0.5>N/A</font> | <font size=0.5></font> |

```python
from sklearn.preprocessing import MinMaxScaler,StandardScaler

## 1.0 归一化
scaler=MinMaxScaler() #实例化
scaler=scaler.fit(data) # fit，本质生成min(x),max(x)
result=scaler.transform(data) # 通过接口导出结果
result

result1_=scaler1.fit_transform(data) ## 训练、导出结果一步达成
result1_

scaler1.inverse_transform(result_) ## 逆转结果

# 使用MinMaxScaler的参数feature_range实现数据归一化到[0,1]以外的范围
scaler=MinMaxScaler(feature_range=[5,10])
result=scaler.fit_transform(data)
result

## 数据特征多时，fit会报错，表示数据量太大无法计算，
## 可以用partial_fit作为训练接口
## scaler=scaler.partial_fit(data),result=scaler.transform(data)

### 使用numpy实现归一化
import numpy as np
x=np.array([[-1,2],[-0.5,6],[0,10],[1,18]])
x_nor=(x-x.min(axis=0))/(x.max(axis=0)-x.min(axis=0))
x_nor 
# 逆转归一化
x_returned=x_nor*(x.max(axis=0)-x.min(axis=0))+x.min(axis=0)
x_returned

## 2.0 标准化
scaler=StandardScaler() # 实例化
scaler.fit(data) # fit，本质生成方差和均值
x_std=scaler.transform(data) #通过接口导出结果
print(scaler.mean_)
print(scaler.var_)

x_std.mean()
x_std.std()

scaler.fit_transform(data)  ## 训练、导出结果一步达成
scaler.inverse_transform(x_std) ## 逆转结果
```

## 2.2 缺失值

**缺失值的方法论：**

<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200331212637629.png" alt="image-20200331212637629" style="zoom:40%;" />

**处理缺失值的方式：**

<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200331212715384.png" alt="image-20200331212715384" style="zoom:50%;" />

![image-20200401095903224](/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200401095903224.png)

**sklearn中缺失值处理方式**：*class sklearn.impute.SimpleImputer (missing_values=nan, strategy=’mean’, fill_value=None, verbose=0, copy=True)*

| 参数           | 含义&输入                                                    |
| -------------- | ------------------------------------------------------------ |
| missing_values | 告诉SimpleImputer，数据中的缺失值长什么样，默认空值np.nan    |
| strategy       | 我们填补缺失值的策略，**默认均值**。 <br>输入“mean”使用均值填补(仅对数值型特征可用) <br>输入“median"用中值填补(仅对数值型特征可用)<br>输入"most_frequent”用众数填补(对数值型和字符型特征都可用) <br>输入“constant"表示请参考参数“fill_value"中的值(对数值型和字符型特征都可用) |
| fill_value     | 当参数startegy为”constant"的时候可用，可输入字符串或数字表示要填充的值，常用0 |
| copy           | 默认为True，将创建特征矩阵的副本，反之则会将缺失值填补到原本的特征矩阵中去。 |

```python
import pandas as pd 
from sklearn.impute import SimpleImputer # 缺失值处理模块
# index_col=0表示作为行标签的列
data=pd.read_csv(r"/Users/songhaiyue/Documents/课件/03数据预处理和特征工程/Narrativedata【瑞客论坛 www.ruike1.com】.csv",index_col=0)
data.head()

# 可以看到Age、Sembarked有缺失
data.info()

# 1、处理Age
Age=data.loc[:,"Age"].values.reshape(-1,1) ## sklearn中特征矩阵必须是二维，利用reshape升维
Age[:10]

## 实例化
imp_mean=SimpleImputer() ##实例化，用均值填补
imp_median=SimpleImputer(strategy="median") # 实例化，用中位数填补
imp_0=SimpleImputer(strategy="constant",fill_value=0) # 实例化，用0填补

imp_mean=imp_mean.fit_transform(Age)
imp_median=imp_median.fit_transform(Age)
imp_0=imp_0.fit_transform(Age)

data.loc[:,"Age"]=imp_median 
data.info()

# 使用众数填补Embarked 
Embarked=data.loc[:,"Embarked"].values.reshape(-1,1)
imp_mode=SimpleImputer(strategy="most_frequent")
data.loc[:,"Embarked"]=imp_mode.fit_transform(Embarked)
data.info()
```

**使用Pandas和Numpy处理缺失值**：*a.isnull()、a.fillna() 、a.dropna(axis=0,inplace=True)*

```python
import pandas as pd 
import numpy as np
# index_col=0表示作为行标签的列
data=pd.read_csv(r"/Users/songhaiyue/Documents/课件/03数据预处理和特征工程/Narrativedata【瑞客论坛 www.ruike1.com】.csv",index_col=0)
data.head()

# （1）使用中位数进行填补，在DataFrame中直接进行填补
# data.loc[:,"Age"]=data.loc[:,"Age"].fillna(data.loc[:,"Age"].median()) 
#  (2) 使用均值进行填补，在DataFrame中直接进行填补
# data.loc[:,"Age"]=data.loc[:,"Age"].fillna(data.loc[:,"Age"].mean()) 
#  (3) 使用众数进行填补，在DataFrame中直接进行填补
data.loc[:,"Age"]=data.loc[:,"Age"].fillna(data.loc[:,"Age"].mode()[0]) 
#  (4) 拉格朗日插值
    #自定义列向量插值函数
# def ploy(s,n,k=6):
#     y=s[list(range(n-k,n))+list(range(n+1,n+1+k))]#取数
#     y=y[y.notnull()]
#     return lagrange(y.index,list(y))(n)
# # for i in data.columns:
# for i in ["Age"]:
#     for j in range(len(data)):
#         if(data[i].isnull())[j]:
#             data[i][j]=ploy(data[i],j)
data.info()

# .dropna(axis=0)删除所有缺失值的行，.dropna(axis=1)删除所有缺失值的列
# inplace=True表示在原数据集上进行修改，默认false，表示生成一个复制对象
data.dropna(axis=0,inplace=True)
data.info()
```

## 2.3 处理分类型特征：编码与哑变量

机器学习中，大多数算法只能处理数值型数据，不能处理文字， 在sklearn中除了专用处理文字的算法，其他算法在fit时全部要求输入数组或矩阵，不能导入文字型数据，因此，**需要将文字型数据转换为数值型**

<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200402105053455.png" alt="image-20200402105053455" style="zoom:50%;" />

```python
from sklearn.preprocessing import LabelEncoder    ## 标签专用，能够将分类转换为分类数值
from sklearn.preprocessing import OrdinalEncoder  ## 特征专用，将分类特征转换为分类数值，不能导入一维数组
from sklearn.preprocessing import OneHotEncoder   ## 独热编码，哑变量
```

| 编码与哑变量    | 功能                           | 重要参数                                                     | 重要属性                              | 重要接口                                                     |
| --------------- | ------------------------------ | ------------------------------------------------------------ | ------------------------------------- | ------------------------------------------------------------ |
| .LabelEncoder   | 分类标签编码                   | N/A                                                          | .classes_: 查看标签中究竟有多少类别   | fit,<br>transform,<br>fit_transform,<br>inverse_transform    |
| .OrdinalEncoder | 分类特征标签                   | N/A                                                          | .categories_:查看特征中究竟有多少类别 | fit,<br/>transform,<br/>fit_transform,<br/>inverse_transform |
| .OneHotEncoder  | 独热编码，为名义变量创建哑变量 | **cagegories:** 每个特征究竟有哪些类别，默认“auto"即让算法自己判断或者可以输入列表，每个元素都是一个列表，表示每个特征中的不同类别<br>**handle_unknown:** 当输入categories，且算法遇见了categories中没有写明的特征或者类别时，是否报错，默认”error“表示请报错，也可以选择”ignore"表示请无视，此时未注明的特征或者类别的哑变量全部显示为0，逆转时未知特征或类别会被返回None | .categories_:查看特征中究竟有多少     | fit,<br/>transform,<br/>fit_transform,<br/>inverse_transform，<br>**get_feature_names:** 查看生成的哑变量的每一列都是什么特征的什么取值 |

<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200402110106803.png" alt="image-20200402110106803" style="zoom:30%;" />

例子：

```python
import pandas as pd 
import numpy as np
from sklearn.preprocessing import LabelEncoder    ## 标签专用，能够将分类转换为分类数值
from sklearn.preprocessing import OrdinalEncoder  ## 特征专用，将分类特征转换为分类数值
# index_col=0表示作为行标签的列
data=pd.read_csv(r"/Users/songhaiyue/Documents/课件/03数据预处理和特征工程/Narrativedata【瑞客论坛 www.ruike1.com】.csv",index_col=0)
data.head()

## 标签编码
y=data.iloc[:,-1] # 输入为标签，故可以一维数据
le=LabelEncoder() #实例化
le=le.fit(y)      #导入数据
labels=le.transform(y) # transform接口调取结果
le.classes_ # 属性.classes_查看有多少类别

# 导入和调取结果一步到位
le.fit_transform(y)
# 使用inverse_transform可以逆转
le.inverse_transform(labels)
# 处理后的特征赋值
data.iloc[:,-1]=labels
data.head()

## 特征编码
## 正常情况下，如下表达
import pandas as pd 
import numpy as np
from sklearn.preprocessing import LabelEncoder    ## 标签专用，能够将分类转换为分类数值
from sklearn.preprocessing import OrdinalEncoder  ## 特征专用，将分类特征转换为分类数值，不能导入一维数组
# index_col=0表示作为行标签的列
data=pd.read_csv(r"/Users/songhaiyue/Documents/课件/03数据预处理和特征工程/Narrativedata【瑞客论坛 www.ruike1.com】.csv",index_col=0)
data.iloc[:,-1]=LabelEncoder().fit_transform(data.iloc[:,-1])
data.head()

data_=data.copy()
data_[:10]
## 类别数据中有空值时，应用OrdinalEncoder会报错
data_.loc[:,"Age"]=data_.loc[:,"Age"].fillna(data_.loc[:,"Age"].mode()[0])
data_.loc[:,"Embarked"]=data_.loc[:,"Embarked"].fillna(data_.loc[:,"Embarked"].mode()[0])

OrdinalEncoder().fit(data_.iloc[:,1:-1]).categories_ ## 查看每个特征中有多少类别，有空值时会报错
data_.iloc[:,1:-1]=OrdinalEncoder().fit_transform(data_.iloc[:,1:-1])
data_.head()

## 哑变量
## 正常情况下，如下表达
import pandas as pd 
import numpy as np
from sklearn.preprocessing import LabelEncoder    ## 标签专用，能够将分类转换为分类数值
from sklearn.preprocessing import OrdinalEncoder  ## 特征专用，将分类特征转换为分类数值，不能导入一维数组
from sklearn.preprocessing import OneHotEncoder   ## 独热编码，哑变量
# index_col=0表示作为行标签的列
data=pd.read_csv(r"/Users/songhaiyue/Documents/课件/03数据预处理和特征工程/Narrativedata【瑞客论坛 www.ruike1.com】.csv",index_col=0)
data.iloc[:,-1]=LabelEncoder().fit_transform(data.iloc[:,-1])
data.info()

## 有空值时，会报错
data.loc[:,"Age"]=data.loc[:,"Age"].fillna(data.loc[:,"Age"].mode()[0])
data.loc[:,"Embarked"]=data.loc[:,"Embarked"].fillna(data.loc[:,"Embarked"].mode()[0])
data.info()

## Sex、embarked是名义变量，用哑变量处理
X=data.iloc[:,1:-1]
enc=OneHotEncoder(categories="auto").fit(X)
result=enc.transform(X).toarray()
result #数组，哑变量形式

## 依然可以一步到位
OneHotEncoder(categories='auto').fit_transform(X).toarray()

pd.DataFrame(enc.inverse_transform(result))
# 哑变量后的五列特征名
enc.get_feature_names()

# axis=1,跨行进行合并，即两表左右相连；axis=0,两表上下相连
newdata=pd.concat([data,pd.DataFrame(result,columns=enc.get_feature_names())],axis=1)
newdata.head()

newdata.drop(["Sex","Embarked"],axis=1,inplace=True)
newdata.head()
```

## 2.4 处理连续型特征：二值化和分段

sklearn中常用的两种：

```python
from sklearn.preprocessing import Binarizer ## 连续变量二值化
from sklearn.preprocessing import KBinsDiscretizer ## 连续变量分箱
```

对于precessing.KBinsDiscretizer: 

| 参数       | 含义&输入                                                    |
| ---------- | ------------------------------------------------------------ |
| n_bins     | 每个特征中分箱的个数，默认5，一次会运用到所有导入的特征      |
| encode     | 编码方式，默认“onehot"<br>"onehot": 哑变量，返回一个稀疏矩阵，每列是一个特征中的一个类别，含有该类别的样本表示为1，不含的表示为0<br>"ordinal": 每个特征的每个箱都被编码为一个整数，返回每一列是一个特征，每个特征下含有不同整数编码的箱的矩阵<br>"onehot-dense":哑变量，返回一个密集数组 |
| strategy   | 用来定义箱宽的方式，默认”quantile"<br>"uniform":等宽分箱，即每个特征的每个箱的最大值之间的差（特征.max() - 特征.min())/n_bins <br>"quantile":等位分箱，即每个特征中的每个箱内的样本数量都相同<br>"kmeans":按聚类分箱，每个箱中的值到最近的一维k均值聚类的簇心的距离都相同 |
| **属性**   | **含义&输入**                                                |
| n_bins_    | 分箱数目                                                     |
| bin_edges_ | 每个箱的边界                                                 |

```python
#************************************1、二值化************************************
import pandas as pd 
import numpy as np
from sklearn.preprocessing import LabelEncoder    ## 标签专用，能够将分类转换为分类数值
from sklearn.preprocessing import OrdinalEncoder  ## 特征专用，将分类特征转换为分类数值，不能导入一维数组
from sklearn.preprocessing import OneHotEncoder   ## 独热编码，哑变量
from sklearn.preprocessing import Binarizer       ## 连续变量二值化
# index_col=0表示作为行标签的列
data=pd.read_csv(r"/Users/songhaiyue/Documents/课件/03数据预处理和特征工程/Narrativedata【瑞客论坛 www.ruike1.com】.csv",index_col=0)
data.iloc[:,-1]=LabelEncoder().fit_transform(data.iloc[:,-1])

## 有空值时，会报错
data.loc[:,"Age"]=data.loc[:,"Age"].fillna(data.loc[:,"Age"].mode()[0])
data.loc[:,"Embarked"]=data.loc[:,"Embarked"].fillna(data.loc[:,"Embarked"].mode()[0])
data.info()

# 将年龄二值化
data_2=data.copy()
x=data_2.iloc[:,0].values.reshape(-1,1)
transformer=Binarizer(threshold=30).fit_transform(x)
transformer.sum()
transformer

## 二值化后数据带入
data_2.iloc[:,0]=transformer
data_2.head()

## 对标签二值化
data_2.iloc[:,3]=Binarizer(threshold=1).fit_transform(data_2.iloc[:,3].values.reshape(-1,1))
data_2.head()

#************************************2、分箱************************************
import pandas as pd 
import numpy as np
from sklearn.preprocessing import LabelEncoder    ## 标签专用，能够将分类转换为分类数值
from sklearn.preprocessing import OrdinalEncoder  ## 特征专用，将分类特征转换为分类数值，不能导入一维数组
from sklearn.preprocessing import OneHotEncoder   ## 独热编码，哑变量
from sklearn.preprocessing import Binarizer       ## 连续变量二值化
from sklearn.preprocessing import KBinsDiscretizer ## 连续变量分箱
# index_col=0表示作为行标签的列
data=pd.read_csv(r"/Users/songhaiyue/Documents/课件/03数据预处理和特征工程/Narrativedata【瑞客论坛 www.ruike1.com】.csv",index_col=0)
data.iloc[:,-1]=LabelEncoder().fit_transform(data.iloc[:,-1])

## 有空值时，会报错
data.loc[:,"Age"]=data.loc[:,"Age"].fillna(data.loc[:,"Age"].mode()[0])
data.loc[:,"Embarked"]=data.loc[:,"Embarked"].fillna(data.loc[:,"Embarked"].mode()[0])
data.info()

x=data.iloc[:,0].values.reshape(-1,1)
est=KBinsDiscretizer(n_bins=3,encode='ordinal',strategy='uniform')
est.fit_transform(x) # 二维数组

# 查看转换后分的箱，变成一列中的三箱
# ravel是降维
set(est.fit_transform(x).ravel())

est=KBinsDiscretizer(n_bins=3,encode='onehot',strategy='uniform')
est.fit_transform(x).toarray()

##  查看分类边界
est.bin_edges_
```

# 3、特征选择 feature_selection

特征工程包括：

| 特征提取<br>（feature extraction)                            | 特征创造<br>(feature creation)                               | 特征选择<br>(feature selection)                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 从文字、图像、声音等其他非结构化数据中提取新信息作为特征。<br>比如：从淘宝宝贝的名称中提取产品类别，如颜色、是否网红等 | 把现有特征进行组合，或互相计算得到新的特征<br>比如：一列特征是速度，一列特征是距离，二者相除得到新的特征时间 | 从所有特征中，选择出有意义、对模型有帮助的特征，避免将所有特征都导入模型去训练的情况 |

**做特征选择之前，一定要跟数据提供者开会，充分了解数据、理解业务**

## 3.1 Filter过滤法

过滤方法通常用作**预处理步骤**，特征选择完全独立于任何机器学习算法，**根据各种统计检验中的分数、相关性各项指标来选择特征**

<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200403095557353.png" alt="image-20200403095557353" style="zoom:50%;" />

![image-20200402210002124](/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200402210002124.png)

| 类                     | 说明                                                        | 超参数的选择                                                 |
| ---------------------- | ----------------------------------------------------------- | ------------------------------------------------------------ |
| VarianceThreshold      | 方差过滤，可输入方差阈值，返回方差大于阈值的新特征矩阵      | 看具体数据究竟是含有更多噪音还是更多有效特征，一般用0、1筛选<br>也可以画学习曲线、取中位数来跑模型确认 |
| SelectKBest            | 选取K个统计量结果最佳的特征，生成符合统计量要求的新特征矩阵 | 看配合使用的统计量                                           |
| chi2                   | 卡方检验，专用于分类算法，捕捉相关性                        | 追求p值小于显著水平的特征                                    |
| f_classif              | F检验分类，**只能捕捉线性相关性**，要求**数据服从正态分布** | 追求p值小于显著水平的特征                                    |
| f_regression           | F检验回归，**只能捕捉线性相关性**，要求**数据服从正态分布** | 追求p值小于显著水平的特征                                    |
| mutual_info_classif    | 互信息分类，**可以捕捉任何相关性**，**不能用于稀疏矩阵**    | 追求互信息估计大于0的特征                                    |
| mutual_info_regression | 互信息回归，**可以捕捉任何相关性**，**不能用于稀疏矩阵**    | 追求互信息估计大于0的特征                                    |



### 方差过滤 sklearn.feature_selection.VarianceThreshold

根据特征本身的特征筛选特征的类

*sklearn.feature_selection.VarianceThreshold​*的重要参数**threshold**，阈值，表示舍弃$方差<threshold$的特征，默认为0

threshold可以通过画学习曲线选择最佳的点，通常使用阈值为0或者很小的值进行方差过滤，消除明显不用的特征，而后在选择其他的特征选择方法继续削减特征数量

```python
## 导入数据
import pandas as pd
data=pd.read_csv(r"/Users/songhaiyue/Documents/课件/03数据预处理和特征工程/digit recognizor【瑞客论坛 www.ruike1.com】.csv")
data.head()

x=data.iloc[:,1:]
y=data.iloc[:,0]
x.shape #42000行，784个特征
"""
数据量比较大，纬度高，如果用支持向量机、神经网络，会跑不出来；使用KNN跑一次大概需要半个小时
"""

from sklearn.feature_selection import VarianceThreshold ##方差过滤模块

## 1、删除threshold<=0的特征
selector=VarianceThreshold()  # 实例化，不填默认方差为0，即删除方差≤0的特征
x_var0=selector.fit_transform(x)  # 获取删除不合格特征之后的新特征矩阵
x_var0.shape ##干掉76个特征，剩余708个特征
## 获取保留特征的名称
x.columns[VarianceThreshold().fit(x).get_support(indices=False)]

## 2、删除threshold<=方差中位数的特征
x_fsvar=VarianceThreshold(x.var().median()).fit_transform(x) ## 利用方差的中位数，减少一半特征
x_fsvar.shape  ## 42000行，392个特征
## 获取保留特征的名称
x.columns[VarianceThreshold(x.var().median()).fit(x).get_support(indices=False)]

## 3、只要方差从大到小，topN的特征
x_fsvar1=VarianceThreshold(x.var().sort_values(ascending=False)[50]).fit_transform(x) ## 利用方差的中位数，减少一半特征
x_fsvar1.shape  ## 42000行，50个特征
## 获取保留特征的名称
x.columns[VarianceThreshold(x.var().sort_values(ascending=False)[50]).fit(x).get_support(indices=False)]

## 4、特征是二分类时
## 当特征是二分类时，特征取值是伯努利随机变量，var=p*(1-p)
# 假设p=0.8，即某种分类占比80%以上时删除特征
x_bvar=VarianceThreshold(0.8*(1-0.8)).fit_transform(x)
x_bvar.shape  ## 42000行，685个特征
## 获取保留特征的名称
x.columns[VarianceThreshold(0.8*(1-0.8)).fit(x).get_support(indices=False)]
```

方差过滤对模型的影响：

```python
from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import VarianceThreshold ##方差过滤模块
import numpy as np
import pandas as pd

## 导入数据
data=pd.read_csv(r"/Users/songhaiyue/Documents/课件/03数据预处理和特征工程/digit recognizor【瑞客论坛 www.ruike1.com】.csv")
data.head()

x=data.iloc[:,1:]
y=data.iloc[:,0]
x_fsvar=VarianceThreshold(x.var().median()).fit_transform(x)
x_fsvar.shape

# python中的魔法命令，可以直接使用%timeit来计算运行这个cell中的代码所需的时间 
# 为了计算所需的时间，需要将这个cell中的代码运行很多次(通常是7次)后求平均值
# 因此运行%%timeit的时间会 远远超过cell中的代码单独运行的时间

#======KNN【TIME WARNING:35mins +】======# 
# KNN：全量特征
cross_val_score(KNN(),x,y,cv=5).mean()
## KNN：全量特征，查看模型运行时间
%timeit cross_val_score(KNN(),x,y,cv=5).mean()
# KNN：特征选择后
cross_val_score(KNN(),x_fsvar,y,cv=5).mean()
# KNN：特征选择后，查看模型运行时间
%timeit cross_val_score(KNN(),x_fsvar,y,cv=5).mean()
# 随机森林：全量特征
cross_val_score(RFC(n_estimators=10,random_state=0),x,y,cv=5).mean()
# 随机森林：全量特征，查看模型运行时间
%timeit cross_val_score(RFC(n_estimators=10,random_state=0),x,y,cv=5).mean()
# 随机森林：特征选择后
cross_val_score(RFC(n_estimators=10,random_state=0),x_fsvar,y,cv=5).mean()
# 随机森林：特征选择后，查看模型运行时间
%timeit cross_val_score(RFC(n_estimators=10,random_state=0),x_fsvar,y,cv=5).mean()
```

结果分析：

- 特征选择后，KNN、随机森林的模型精确度均提高了，说明被过滤掉的特征大部分是噪音

- KNN的模型效果比随机森林略好，可以调整随机森林的n_estimators提高模型效果
- KNN在特征选择前后，计算效率显著提高，这是因为KNN本身是升维的算法
- 随机森林在特征选择前后的计算效率变化不大，因为随机森林每次只是选择10-20个特征

方差过滤的影响如下：

|              | threshold小，被过滤掉的特征少                                | threshold大，被过滤掉的特征多                                |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **模型表现** | 不会有太大影响                                               | 若变好，代表被过滤掉的特征大部分是噪音<br>若变差，代表被过滤掉的特征大部分是有效特征 |
| **运行时间** | 若方差小的特征多，可能降低模型的运行时间<br>若方差小的特征少，对模型计算效率影响不大 | 一定能够降低模型运行时间，算法在遍历特征时的计算越复杂，运行时间下降的越多 |

### 相关性过滤

```python
## 卡方过滤，在方差过滤的基础上进行
from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import VarianceThreshold ##方差过滤模块
from sklearn.feature_selection import SelectKBest  
from sklearn.feature_selection import chi2 # 卡方检验类
import numpy as np
import pandas as pd

## 导入数据
data=pd.read_csv(r"/Users/songhaiyue/Documents/课件/03数据预处理和特征工程/digit recognizor【瑞客论坛 www.ruike1.com】.csv")
data.head()

## 进行方差过滤，
x=data.iloc[:,1:]
y=data.iloc[:,0]
x_fsvar=VarianceThreshold(x.var().median()).fit_transform(x)
x_fsvar.shape

##***********************1、卡方过滤*********************** 
## 方差过滤是交叉验证分数提升，故在方差过滤后的特征上进行卡方过滤
x_fschi=SelectKBest(chi2,k=300).fit_transform(x_fsvar,y)
x_fschi.shape

# 验证模型效果，效果降低，说明k=300不合适
cross_val_score(RFC(n_estimators=10,random_state=0),x_fschi,y,cv=5).mean()

## 方差过滤是交叉验证分数提升，故在方差过滤后的特征上进行卡方过滤
x_fschi=SelectKBest(chi2,k=370).fit_transform(x_fsvar,y)
x_fschi.shape
# 验证模型效果，效果降低，说明k=300不合适
cross_val_score(RFC(n_estimators=10,random_state=0),x_fschi,y,cv=5).mean()

## 选取超参数K：1、看学习曲线
# 随着K值增加，模型表现上升，说明K越大越好
%matplotlib inline #图片内嵌显示
import matplotlib.pyplot as plt 
score=[]
for i in range(390,200,-10):
    x_fschi=SelectKBest(chi2,k=i).fit_transform(x_fsvar,y)
    once=cross_val_score(RFC(n_estimators=10,random_state=0),x_fschi,y,cv=5).mean()
    score.append(once)
plt.plot(range(390,200,-10),score)
plt.show()

## 选取超参数K：2、看p值
chivalue,pvalues_chi=chi2(x_fsvar,y)
chivalue
pvalues_chi

## k取多少？p值大于某设定值，如0.05，0.01
## 所有方差过滤后的特征都被保存了
k=chivalue.shape[0]-(pvalues_chi>0.05).sum() ##加和的是布尔值
print(k)
x_fschi=SelectKBest(chi2,k=k).fit_transform(x_fsvar,y)
print(cross_val_score(RFC(n_estimators=10,random_state=0),x_fschi,y,cv=5).mean())

##***********************2、F检验*********************** 
from sklearn.feature_selection import f_classif
F,pvalues_f=f_classif(x_fsvar,y)
F
pvalues_f
k=F.shape[0]-(pvalues_f>0.05).sum()
k
## 仍然无法去除任何特征

##***********************3、互信息法***********************
from sklearn.feature_selection import mutual_info_classif as MIC 
result=MIC(x_fsvar,y)
k=result.shape[0]-sum(result<=0)
k
# 仍然无法去除任何特征
```

## 3.2 Embedded嵌入法

<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200403095537606.png" alt="image-20200403095537606" style="zoom:50%;" />



<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200403165701844.png" alt="image-20200403165701844" style="zoom:50%;" />

***class sklearn.feature_selection.SelectFromModel (estimator, threshold=None, prefit=False, norm_order=1, max_features=None)***

| 参数         | 说明                                                         |
| ------------ | ------------------------------------------------------------ |
| estimator    | 模型评估器，带有feature_importance_、coef属性或带有L1、L2惩罚项的模型可以使用 |
| threshold    | 特征重要性的阈值，低于此阈值的特征被删除                     |
| prefit       | 默认False，判断是否将实例化后的模型直接传递给构造函数；<br>若为True，则必须直接调用fit和transform，不能使用fit_transform，并且SelectFromModel不能与cross_val_score、GridSearchCV和克隆估计器的类似实用程序一起使用 |
| norm_order   | k可输入非零整数，正无穷，负无穷，默认1<br>在评估器的coef_属性高于一维的情况下，用于过滤低于阈值的系数的向量的范数的阶数 |
| max_features | 阈值设定下，要选择的最大特征数<br>要禁用阈值并仅根据max_features选择，请设置threshold=-np.inf |

**以随机森林为例：**

```python
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier as RFC 
import pandas as pd
from sklearn.model_selection import cross_val_score
import numpy as np
import matplotlib.pyplot as plt

## 导入数据
data=pd.read_csv(r"/Users/songhaiyue/Documents/课件/03数据预处理和特征工程/digit recognizor【瑞客论坛 www.ruike1.com】.csv")
data.head()

x=data.iloc[:,1:]
y=data.iloc[:,0]
x.shape

FC_=RFC(n_estimators=10,random_state=0) ##随机森林实例化
x_embedded=SelectFromModel(RFC_,threshold=0.005).fit_transform(x,y) ## 嵌入法实例化
x_embedded.shape
# 在这里只想取出有限的特征，0.005对于784个特征的数据而言是非常高的阈值，
# 因为每个特征只能够分到0.001的feature_importantce_
# 仅剩47个特征，模型的维度明显降低了

# 通过学习曲线寻找最佳阈值
%matplotlib inline
score=[]
threshold=np.linspace(RFC_.fit(x,y).feature_importances_.min(),RFC_.fit(x,y).feature_importances_.max(),20)
for i in threshold:
    x_embedded=SelectFromModel(RFC_,threshold=i).fit_transform(x,y)
    once=cross_val_score(RFC_,x_embedded,y,cv=5).mean()
    score.append(once)
plt.plot(threshold,score)
plt.show()

# 细化学习曲线，找到最高点
%matplotlib inline
score=[]
threshold=np.linspace(0,0.001,20)
for i in threshold:
    x_embedded=SelectFromModel(RFC_,threshold=i).fit_transform(x,y)
    once=cross_val_score(RFC_,x_embedded,y,cv=5).mean()
    score.append(once)
plt.plot(threshold,score)
plt.show()

## 认为0.0005是最优点
RFC_=RFC(n_estimators=10,random_state=0)
x_embedded=SelectFromModel(RFC_,threshold=0.0005).fit_transform(x,y)
x_embedded.shape
cross_val_score(RFC_,x_embedded,y,cv=5).mean()
## 效果比方差过滤略好，所用特征347个,93.88%

## 对随机森林进行调参
RFC_=RFC(n_estimators=100,random_state=0)
x_embedded=SelectFromModel(RFC_,threshold=0.0005).fit_transform(x,y)
cross_val_score(RFC_,x_embedded,y,cv=5).mean()
## 模型效果提升明显，约3pp
## 也可以画学习曲线选择最优n_estimators
## 96.4%
```

<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200403171703282.png" alt="image-20200403171703282" style="zoom:50%;" /><img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200403171731916.png" alt="image-20200403171731916" style="zoom:50%;" />

## 3.3 Wrapper包装法



<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200403160629615.png" alt="image-20200403160629615" style="zoom:50%;" />

<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200403172312483.png" alt="image-20200403172312483" style="zoom:50%;" />

***class sklearn.feature_selection.RFE (estimator,n_features_to_select=None,step=1,verbose=0)***

| 参数       | 说明                                                         |
| ---------- | ------------------------------------------------------------ |
| estimator  | 实例化后的评估器                                             |
| threshold  | 想要选择的特征个数                                           |
| prefit     | 每次迭代中希望移除的特征个数                                 |
| norm_order | **说明**                                                     |
| .support_  | 返回所有特征是否被选中的布尔矩阵                             |
| .ranking_  | 返回特征的按数次迭代中综合重要性的排名，若重要性相同，则排名相同 |

**以随机森林为例：**

```python
from sklearn.feature_selection import RFE 
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier as RFC 
import pandas as pd
from sklearn.model_selection import cross_val_score
import numpy as np
import matplotlib.pyplot as plt

## 导入数据
data=pd.read_csv(r"/Users/songhaiyue/Documents/课件/03数据预处理和特征工程/digit recognizor【瑞客论坛 www.ruike1.com】.csv")
data.head()

x=data.iloc[:,1:]
y=data.iloc[:,0]
x.shape

RFC_=RFC(n_estimators=10,random_state=0) #随机森林实例化
selector=RFE(RFC_,n_features_to_select=340,step=50).fit(x,y) # RFE实例化
selector.support_ # 返回所有特征是否被选中的不二矩阵

selector.support_.sum() ## 选中了340个特征
selector.ranking_ # 重要性相同，则排序相同

# 查看模型效果
cross_val_score(RFC_,x_wrapper,y,cv=5).mean()

## 通过学习曲线寻找最优的n_features_to_select 
%matplotlib inline
score=[]
RFC_=RFC(n_estimators=10,random_state=0) #随机森林实例化
for i in range(1,751,50):
    x_wrapper=RFE(RFC_,n_features_to_select=i,step=50).fit_transform(x,y)
    once=cross_val_score(RFC_,x_wrapper,y,cv=5).mean()
    score.append(once)
plt.figure(figsize=[20,5])
plt.plot(range(1,751,50),score)
plt.xticks(range(1,751,50))
plt.show()

## 51个特征时，交叉验证的效果已经达到90%以上

```

<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200403173105519.png" alt="image-20200403173105519" style="zoom:50%;" />

## 3.4 特征选择总结

过滤法更快、更粗糙，包装法嵌入法更精确，比较适合具体到算法去调整，但计算量大，运行时间长

当数据量大时，优先使用方差过滤和互信息法调整，在上其他特征选择方法

逻辑回归优先使用嵌入法，SVM优先使用包装法，迷茫的时候，从过滤法开始，看具体数据具体分析

