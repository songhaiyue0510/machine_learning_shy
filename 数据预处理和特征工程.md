[TOC]



# 1、概述

## 1.1 数据预处理与特征工程

| 数据挖掘的五大流程                                           |
| ------------------------------------------------------------ |
| 1、获取数据<br>**2、数据预处理<br>**（1）从数据中检测、纠正或者删除损坏、不准确、不适用模型的记录的过程<br>（2）可能面对的问题：<br>          *  数据类型不同，文字、数字、含时间序列、连续、离散等<br>          *  数据质量不行，噪声、异常、缺失、出错、量纲不一、数据是偏态、数据量太大或太小<br>（3）目的：让数据适应模型，匹配模型的需求<br>**3、特征工程**<br>（1）原始数据→更能代表预测模型的潜在问题的特征的过程，可以通过挑选最相关的特征、提取特征、创造特征（降维算法）实现<br>（2）可能面对的问题：特征之间有相关性、特征和标签无关、特征太多或太少、特征无法表现出应有的数据现象或真实面貌<br>（3）目的：降低计算成本，提升模型上限<br>4、建模，测试模型并预测出结果<br>5、上线，验证模型结果 |

## 1.2 sklearn 中的数据处理和特征工程

<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200331093449173.png" alt="image-20200331093449173" style="zoom:50%;" />

- 模块Preocessing：几乎包含预处理的所有内容
- 模块Impute: 填补缺失值专用
- 模块feature_selection：包含特征选择的各种方法的时间
- 模块decomposition：包含降维算法

# 2、数据预处理Processing & Impute

## 2.1 无量纲化

无量纲化：即不同规格的数据转换到统一规格，或不同分布的数据转换到某个特定分布

- 以梯度和矩阵为核心的算法中，如逻辑回归、支持向量机、神经网络等，无量纲化可以**加快求解速度**
- 在距离类模型中，如KNN、K-Means中，无量纲化可以**提升模型精度**
- 决策树和树的集成算法们，不需要无量纲化

$$
无量纲化
\begin{cases}
线性无量纲化
\begin{cases}
中心化（zero-centerd、mean-subtraction):所有记录减去一个值，即将所有数据评议到某个位置\\
缩放处理（scale)：所有记录除以一个固定值，将数据固定在一个范围之中，取对数也是一种缩放处理
\end{cases}\\
非线性无量纲化
\end{cases}
$$

<font color=#0000FF size=2 face="黑体">***常用的两种：***</font>

**数据归一化**（Preprocessing.MinMaxScaler）:  $x^*=\frac {x-min(x)}{max(x)-min(x)}$，归一化后数据服从正态分布，默认压缩到[0,1]，易受异常值影响

**数据标准化**（preprocessing.standardScaler)：$x^*=\frac {x-\mu}{\sigma}$，标准化后数据服从标准正态分布（均值0、方差1）

* 空值NaN会被当做缺失值，在fit时忽略，在transform时保持NaN的状态显示
* 特征矩阵X至少为2维矩阵
* 通常选择StandardScaler进行特征缩放（效果不好换MinMaxScaler），在PCA、聚类、逻辑回归、神经网络等优先选择
* 在不涉及距离度量、梯度、协方差计算以及数据需要被压缩到特定区间时，使用MinMaxScaler

<font color=#0000FF size=2 face="黑体">***无量纲化手段：：***</font>

| <font size=1>无量纲化</font> | <font size=1>功能</font> | <font size=1>中心化</font> | <font size=1>缩放</font> | 详解                                                         |
| ------| --------- | ---------- | ------------ | ------------------------------------------------------------ |
| <font size=0.8>.StandardScaler</font> | <font size=0.5>标准化</font> | <font size=0.5>均值</font> | <font size=0.5>方差</font> | <font size=0.5>通过减掉均值并缩放到单位方差来标准化特征，标准化后服从标准正态分布（方差1均值0）</font> |
| <font size=0.8>.MinMasScaler</font> | <font size=0.5>归一化</font> | <font size=0.5>最小值</font> | <font size=0.5>极差</font> | <font size=0.5>通过最大值最小值将每个特征缩放到给定范围，默认[0,1]</font> |
| <font size=0.8>.MaxAbsScaler</font> | <font size=0.5>缩放</font> | <font size=0.5>N/A</font>  | <font size=0.5>最大值</font> | <font size=0.5>通过除以特征中**绝对值最大的数值的绝对值**，将数据压缩到[-1,1]之间，无中心化，<br>**不会破坏数据的稀疏性**，即对数据中取值为0的数据不影响 </font> |
| <font size=0.8>.RobustScaler</font> | <font size=0.5>无量纲化</font> | <font size=0.5>中位数</font> | <font size=0.5>四分位数范围</font> | <font size=0.5>可以处理异常值，对异常值不敏感的统计量来缩放数据<br>这个缩放器减去中位数并除以百分位数范围（IQR: Interquartile Range）缩放数据，IQR为第一分位数（25%）和第三分位数（75%）之间的范围<br>**当异常值很多、噪音很大时，此方法效果较好**</font> |
| <font size=0.8>.Normalizer</font> | <font size=0.5>无量纲化</font> | <font size=0.5>N/A</font> | <font size=0.5>sklearn中未明确，依范数原理：<br>l1:样本向量的长度/样本中每个元素的绝对值的和<br>l2: 样本向量的长度/样本中每个元素的欧氏距离</font> | <font size=0.5>将样本缩放到单位范数<br>将输入的数据缩放到单位范数是文本分类或者聚类的常见操作<br>使用参数norm来确定正则化的番薯方向，可以选择”l1"、"l2"、"max"三种选项，默认l2范数<br>这个评估器的fit接口什么也不做，但在管道中依然有用</font> |
| <font size=0.8>.PowerTransformer</font> | <font size=0.5>非线性无量纲化</font> | <font size=0.5>N/A</font> | <font size=0.5>N/A</font> | <font size=0.5>应用特征功率变换使数据更接近正态分布<br>功率变换使一系列参数单调变换，使数据更像高斯，对于建模与异方差性、活其他需要正态性的情况非常有用<br>要求数据严格为正，power_transform()通过最大似然估计来稳定方差并确定最小化偏度的最佳参数<br>默认情况下，标准化应用于转换后的数据</font> |
| <font size=0.8>.QuantileTransformer</font> | <font size=0.5>非线性无量纲化</font> | <font size=0.5>N/A</font> | <font size=0.5>N/A</font> | <font size=0.5>使用百分位转换特征，通过缩小边缘异常值和非异常值之间的距离来提供特征的非线性变换，可以使用参数output_distribution="normal"来将数据映射到标准正态分布</font> |
| <font size=0.8>.KernelCenterer</font> | <font size=0.5>中心化</font> | <font size=0.5>均值</font> | <font size=0.5>N/A</font> | <font size=0.5></font> |

```python
from sklearn.preprocessing import MinMaxScaler,StandardScaler

## 1.0 归一化
scaler=MinMaxScaler() #实例化
scaler=scaler.fit(data) # fit，本质生成min(x),max(x)
result=scaler.transform(data) # 通过接口导出结果
result

result1_=scaler1.fit_transform(data) ## 训练、导出结果一步达成
result1_

scaler1.inverse_transform(result_) ## 逆转结果

# 使用MinMaxScaler的参数feature_range实现数据归一化到[0,1]以外的范围
scaler=MinMaxScaler(feature_range=[5,10])
result=scaler.fit_transform(data)
result

## 数据特征多时，fit会报错，表示数据量太大无法计算，
## 可以用partial_fit作为训练接口
## scaler=scaler.partial_fit(data),result=scaler.transform(data)

### 使用numpy实现归一化
import numpy as np
x=np.array([[-1,2],[-0.5,6],[0,10],[1,18]])
x_nor=(x-x.min(axis=0))/(x.max(axis=0)-x.min(axis=0))
x_nor 
# 逆转归一化
x_returned=x_nor*(x.max(axis=0)-x.min(axis=0))+x.min(axis=0)
x_returned

## 2.0 标准化
scaler=StandardScaler() # 实例化
scaler.fit(data) # fit，本质生成方差和均值
x_std=scaler.transform(data) #通过接口导出结果
print(scaler.mean_)
print(scaler.var_)

x_std.mean()
x_std.std()

scaler.fit_transform(data)  ## 训练、导出结果一步达成
scaler.inverse_transform(x_std) ## 逆转结果
```

## 2.2 缺失值

**缺失值的方法论：**

<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200331212637629.png" alt="image-20200331212637629" style="zoom:40%;" />

**处理缺失值的方式：**

<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200331212715384.png" alt="image-20200331212715384" style="zoom:50%;" />

<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200331212741312.png" alt="image-20200331212741312" style="zoom:100%;" />

**sklearn中缺失值处理方式**：*class sklearn.impute.SimpleImputer (missing_values=nan, strategy=’mean’, fill_value=None, verbose=0, copy=True)*

| 参数           | 含义&输入                                                    |
| -------------- | ------------------------------------------------------------ |
| missing_values | 告诉SimpleImputer，数据中的缺失值长什么样，默认空值np.nan    |
| strategy       | 我们填补缺失值的策略，**默认均值**。 <br>输入“mean”使用均值填补(仅对数值型特征可用) <br>输入“median"用中值填补(仅对数值型特征可用)<br>输入"most_frequent”用众数填补(对数值型和字符型特征都可用) <br>输入“constant"表示请参考参数“fill_value"中的值(对数值型和字符型特征都可用) |
| fill_value     | 当参数startegy为”constant"的时候可用，可输入字符串或数字表示要填充的值，常用0 |
| copy           | 默认为True，将创建特征矩阵的副本，反之则会将缺失值填补到原本的特征矩阵中去。 |

```python
import pandas as pd 
from sklearn.impute import SimpleImputer # 缺失值处理模块
# index_col=0表示作为行标签的列
data=pd.read_csv(r"/Users/songhaiyue/Documents/课件/03数据预处理和特征工程/Narrativedata【瑞客论坛 www.ruike1.com】.csv",index_col=0)
data.head()

# 可以看到Age、Sembarked有缺失
data.info()

# 1、处理Age
Age=data.loc[:,"Age"].values.reshape(-1,1) ## sklearn中特征矩阵必须是二维，利用reshape升维
Age[:10]

## 实例化
imp_mean=SimpleImputer() ##实例化，用均值填补
imp_median=SimpleImputer(strategy="median") # 实例化，用中位数填补
imp_0=SimpleImputer(strategy="constant",fill_value=0) # 实例化，用0填补

imp_mean=imp_mean.fit_transform(Age)
imp_median=imp_median.fit_transform(Age)
imp_0=imp_0.fit_transform(Age)

data.loc[:,"Age"]=imp_median 
data.info()

# 使用众数填补Embarked 
Embarked=data.loc[:,"Embarked"].values.reshape(-1,1)
imp_mode=SimpleImputer(strategy="most_frequent")
data.loc[:,"Embarked"]=imp_mode.fit_transform(Embarked)
data.info()
```

**使用Pandas和Numpy处理缺失值**：*a.isnull()、a.fillna() 、a.dropna(axis=0,inplace=True)*

```python
import pandas as pd 
import numpy as np
# index_col=0表示作为行标签的列
data=pd.read_csv(r"/Users/songhaiyue/Documents/课件/03数据预处理和特征工程/Narrativedata【瑞客论坛 www.ruike1.com】.csv",index_col=0)
data.head()

data.loc[:,"Age"]=data.loc[:,"Age"].fillna(data.loc[:,"Age"].median()) # 使用中位数进行填补，在DataFrame中直接进行填补
data.info()

# .dropna(axis=0)删除所有缺失值的行，.dropna(axis=1)删除所有缺失值的列
# inplace=True表示在原数据集上进行修改，默认false，表示生成一个复制对象
data.dropna(axis=0,inplace=True)
data.info()
```

