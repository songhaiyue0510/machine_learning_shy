[TOC]



# 1、概述

维度：特征的数量，对于图像而言，就是图像中特征向量的数量

降维：降低特征矩阵中特征的数量

---

特征工程包括：

| 特征提取<br>（feature extraction)                            | 特征创造<br>(feature creation)                               | 特征选择<br>(feature selection)                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 从文字、图像、声音等其他非结构化数据中提取新信息作为特征。<br>比如：从淘宝宝贝的名称中提取产品类别，如颜色、是否网红等 | 把现有特征进行组合，或互相计算得到新的特征<br>比如：一列特征是速度，一列特征是距离，二者相除得到新的特征时间 | 从所有特征中，选择出有意义、对模型有帮助的特征，避免将所有特征都导入模型去训练的情况 |

---

sklearn中降维算法主要在模块decomposition中，主要讲PCA类，奇异值分解SVD和主成分分析PCA都是矩阵分解算法中入门算法，通过汾姐矩阵进行降维，目前LDA是比较火的

<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200405093357218.png" alt="image-20200405093357218" style="zoom:50%;" />

# 2、PCA和SVD

降维过程中，**能够减少特征的数量，又保留大部分有效信息**---将带有重复信息的特征合并，并删除哪些带无效信息的特征，逐渐创造出**代表原特征矩阵大部分信息、特征更少的新特征矩阵**

以PCA为代表的降维算法不具有可读性，属于特征创造，不适用于探索特征和标签之间的关系的模型，在线性回归中，我们都使用特征选择

<font color=#0000FF size=2>**降维的步骤（PCA和SVD都遵循这个步骤）：**</font>

| 步骤 | 二维空间                                                     | n维空间                                                      |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 1    | 输入原数据，结构为（3，2）<br>找到原本的2个特征对应的直角坐标系，本质是找出这2个特征构成的2维平面 | 输入原数据，结构为（m，n）<br/>找到原本的n个特征向量构成的n维空间V |
| 2    | 决定降维后的特征数量：1                                      | 决定降维后的特征数量：k                                      |
| 3    | 旋转，找到一个新的坐标系<br>本质是找出2个新的特征向量，以及它们构成的新2维平面<br>**新特征向量让数据能够被压缩到少数特征上**，并且总信息量损失较少 | 通过某种变化，找到n个新的特征向量，以及它们构成的新n维空间V，**让数据能够压缩到少数特征并且总信息量不损失太多（矩阵分解）** |
| 4    | 找到数据点在新坐标系上，2个新坐标轴上的坐标                  | 找出原始数据在新特征空间V中的n个新特征向量上的对应值，即“将数据映射到新空间上” |
| 5    | 选取第1个方差最大的特征向量，删掉没有被选中的特征，成功将2维平面降为1维 | 选取前k个信息量最大的特征，删掉没有被选中的特征，成功将n维空间V降为k维 |

## 2.1 PCA算法

PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维

| PCA的算法步骤：设有$m$条$n$维数据                            |
| ------------------------------------------------------------ |
| 1、将原始数据按行组成$n$行$m$列矩阵$X_{n\times m}$ <br>2、将X的每一行进行零均值化，即减去这一行的均值<br>3、求出协方差矩阵$C_{n\times n}=\frac{1}{m}XX^T$，协方差矩阵$C$是一个的$n*n$的对称矩阵，对角线为各个变量的方差，其他为每两个特征的协方差<br>4、求出协方差矩阵的特征值和特征向量<br>5、特征向量单位化后按行排列，并按照特征值大小从大到小排列，取前$k$行组成矩阵$P_{k\times n}$<br>6、$Y_{k\times m}=P_{k\times n}X_{n\times m}$即为降维到$k$维后的数据 |

**当特征矩阵零均值化后：**
$$
Var(a)=\frac 1{m}\sum_{i=1}^m(a_i-\mu)^2=\frac 1{m}\sum_{i=1}^ma_i^2 \\
Cov(a,b)=\frac 1{m-1}\sum_{i=1}^m(a_i-\mu_a)(b_i-\mu_b)≈\frac1{m}\sum_{i=1}^ma_ib_i
$$
**降维的优化目标是：**将一组N维向量降为K维，目标是选择K个正交基，当原始数据变换到这组基后协方差为0，方差尽可能大（在正交的约束下，取最大的K个方差）

**矩阵对角化：**根据优化目标，需要将$C$的对角线外的元素化为0，并且使对角线上的元素按大小从上到小排列（变量方差尽可能大）

假设$P$是一组基按行组成的矩阵，$Y=PX$，$Y$为$X$对$P$做基变换后的数据，$Y$的协方差矩阵为D
$$
\begin{equation}\begin{split} 
D&=\frac 1mYY^T\\
 &=\frac 1m(PX)(PX)^T\\
 &=\frac 1mPXX^TP^T\\
 &=P (\frac1mXX^T)P^T\\
 &=PCP^T
\end{split}\end{equation}
$$
**$P$为特征向量单位化后按行排列出的$n*n$矩阵$(e_1,e_2,...,e_n)^T$**



## 2.2 SVD算法

$A$为$m$行$n$列（即$n$个特征）的数据，其中$M$为对角矩阵，对角线元素为奇异值，从大到小排列，不为0，其他元素为0，其中$UU^T=I,VV^T=I$
$$
A_{m\times n}=U_{m\times m}M_{m\times n}V_{n\times n}^T
$$
推导过程：
$$
A^TA=VM^TU^TUMV^T=V(M^TM)V^T\\
AA^T=UMV^TVM^TU^T=U(MM^T)U^T
$$
故$V$是$A^TA$的特征向量，$U$是$AA^T$的特征向量，$M^TM$是有奇异值$\sigma^2$组成的对角矩阵

## 2.3 PCA和SVD的关系

由上述分析可知，

1、PCD关键在于解决协方差矩阵$C=\frac 1{m}XX^T$的特征值分解

2、SVD关键在于$A^TA$的特征值分解

若取$A=\frac {X^T}{\sqrt m}$，则有$A^TA=(\frac {X^T}{\sqrt m})^T(\frac {X^T}{\sqrt m})=\frac 1{m}XX^T$，PCA的问题转化成SVD的问题

其实，PCA只与SVD的右奇异向量的压缩效果相同

**1、若取V的前$k$行作为变换矩阵$P_{k\times n}$，则$Y_{k\times n}=P_{k\times n}X_{n\times m}$，起到压缩行即降维的效果**

2、若取U的前$d$行作为变换矩阵$P_{d\times n}$，则$Y_{n\times d}=X_{n\times m}P_{m\times d}$，起到压缩列即去除冗余样本的效果

无论PCA还是SVD，都需要便利所有的特这个你和样本计算信息量指标，并且在矩阵分解的过程之中，会产生比原来的特征矩阵更大的矩阵，还需要产生协方差矩阵去计算更多的信息，因此，**计算量很大，运行比较缓慢**

## 2.4 PCA和SVD在sklearn应用

在sklearn中，由于PCA中需要进行特征值求解，计算量很大，故内部采用奇异值分解提高效率，根据前面PCA和SVD的算法，$X_{dr}=X_{m\times n}*V_{k\times n}^T$

<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200407152123935.png" alt="image-20200407152123935" style="zoom:90%; transform:rotate(90deg);" />

<font color=#FF0000 size=2>***class sklearn.decomposition.PCA (n_components=None, copy=True, whiten=False, svd_solver=’auto’, tol=0.0,iterated_power=’auto’, random_state=None)***</font>

| 参数                          | 说明                                                         |
| ----------------------------- | ------------------------------------------------------------ |
| n_components                  | 降维后需要的维度，即降维后需要保留的特征数量<br>1、一般为[0,min(X.shape)]中的整数，从为了可视化一组数据观察数据分布，往往把数据降到3维以下，n_components=2比较常见<br>2、"mle"即自选超参数<br>3、svd_solver="full", n_components取[0,1]的值，代表保存的信息量的百分比，PCA会自动选出超过给定值的特征数量 |
| svd_solver                    | 奇异值分解器，用来控制矩阵分解的一些细节，默认”auto“<br><font color=#0000FF size=2>**"auto"：**</font>基于X.shape和n_components的默认策略来选择分解器:如果输入数据的尺寸大于500x500且要提取的特征数小于数据最小维度min(X.shape)的80%，就启用效率更高的”randomized“方法。否则，精确完整的SVD将被计算，截断将会在矩阵被分解完成后有选择地发生<br><font color=#0000FF size=2>**"full"：**</font>从scipy.linalg.svd中调用标准的LAPACK分解器来生成精确完整的SVD，适合数据量比较适中，计算时 间充足的情况，生成的精确完整的SVD的结构为:$U_{m\times m},M_{m\times n},V_{n\times n}^T$<br><font color=#0000FF size=2>**"arpack"：**</font>从scipy.sparse.linalg.svds调用ARPACK分解器来运行阶段奇异值分解（SVD truncated），分解时就将特征数量降到n_components中输入的k值，**可以加快运行速度，适合特征矩阵很大的时候，但是一般用于特征矩阵为稀疏矩阵的情况**，过程中包含**一定的随机性**，截断后的SVD分解出来的结构为：$U_{m\times k},M_{k\times k},V_{k\times  n}^T$<br><font color=#0000FF size=2>**"randomized"：**</font>通过Halko等人的随机方法进行随机SVD。在”full"方法中，分解器会根据原始数据和输入的n_components值去计算和寻找符合需求的新特征量，但在此方法中，分解器会先生成多个随机变量，然后一一去检测这些随机变量中是否有任何一个符合我们的分解去修，如果符合就保留这个随即向量，并基于这个随即向量来构建后续的向量空间，**计算效率高，保证模型运行效果，适合特征矩阵巨大、计算量庞大的情况**<br>一般默认auto，算不出来就选“randomized" |
| random_state                  | 在svd_solver为“arpack"或”randomized"时生效，控制随机性       |
| **属性**                      | **说明**                                                     |
| .components_                  | fit中奇异值分解的结果V(k,n)，其为新特征空间，其他被舍弃了<br>若原特征矩阵是图像，V(k,n)这个空间也可以被可视化，通过两张图对比，就可以看到新特征矩阵提取什么信息 |
| .explained_variance_          | 查看降维后每个新特征向量上所带信息量的大小（可解释性方差的大小） |
| .explained_variance_ratio_    | 查看降维后每个新特征向量所占的信息量占原始数据总信息量的百分比，**pca.explained_variance_ratio_.sum()**查看降维后数据的总信息量占比 |
| **接口**                      | **说明**                                                     |
| inverse_transform             | 将pca之后的结果逆反，仅带有降维后的信息，不能完全恢复数据，**只是将数据重新映射到原数据所在的特征空间中可用来降噪** |
| fit，transfrom，fit_transform |                                                              |

<font color=#0000FF size=2>**示例--鸢尾花数据集：**</font>

```python
import matplotlib.pyplot as plt 
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
import pandas as pd
import numpy as np

iris=load_iris()
y=iris.target
x=iris.data
x.shape ##4维的特征矩阵

# 调用PCA
pca=PCA(n_components=2)
pca=pca.fit(x)
x_dr=pca.transform(x)
x_dr

# 也可以fit_transform一步到位
x_dr=PCA(2).fit_transform(x)
x_dr

# 可视化
plt.figure()
plt.scatter(x_dr[y==0,0],x_dr[y==0,1],c='red',label=iris.target_names[0])
plt.scatter(x_dr[y==1,0],x_dr[y==1,1],c='black',label=iris.target_names[1])
plt.scatter(x_dr[y==2,0],x_dr[y==2,1],c='orange',label=iris.target_names[2])
plt.legend()
plt.title('PCA of IRIS dataset')
plt.show()

## 可视化2
colors=['red','black','orange']
plt.figure()
for i in [0,1,2]:
    plt.scatter(x_dr[y==i,0],x_dr[y==i,1],c=colors[i],label=iris.target_names[i])
plt.legend()
plt.title('PCA of IRIS dataset')
plt.show()

##*********************************************************************************
##===========================探索降维后的数据，重要属性================================
##*********************************************************************************

# 属性explained_variance_,查看降维后每个新特征向量上所带信息量的大小（可解释性方差的大小）
pca.explained_variance_
# 属性explained_variance_ratio_,查看降维后每个新特征向量所占的信息量占原始数据总信息量的百分比
# 又叫做可解释方差贡献率
pca.explained_variance_ratio_
## 大部分信息都被有效的集中在了第一个特征，降维后特征矩阵带有原矩阵信息的百分比
pca.explained_variance_ratio_.sum()


##*********************************************************************************
##*******************************选择最好的n_components******************************
##*********************************************************************************

##==================方法一：累积可解释方差贡献率曲线==================
## 横坐标为降维后保留的特征个数，纵坐标为降维后新特征矩阵捕捉到的可解释方差贡献率
pca_line=PCA().fit(x)  ##内部不填写n_components,默认为min(x.shape)
plt.plot([1,2,3,4],np.cumsum(pca_line.explained_variance_ratio_))
plt.xticks([1,2,3,4]) # 限制坐标轴显示为整数
plt.xlabel("number of components after dimension reduction")
plt.ylabel("cumulative explained variance ratio")
plt.show()
## 2、3都可以，找折点

##==================方法二：自选超参数==================
x_mle=PCA(n_components="mle").fit_transform(x)
x_mle.shape
PCA(n_components="mle").fit(x).explained_variance_ratio_.sum()

##==================方法三：按信息量占比自选超参数================== 
# 输入[0,1]之间的浮点数，且svd_solver="full",表示降维后的总解释方差占比大于给定的百分比
# PCA会自动选出能够保留信息量超过给定值的特征数量
pca_f=PCA(n_components=0.97,svd_solver="full")
pca_f=pca_f.fit(x)
x_f=pca_f.transform(x)
pca_f.explained_variance_ratio_
pca_f.explained_variance_ratio_.sum()
```

<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200406113749436.png" alt="image-20200406113749436" style="zoom:50%;" />



<font color=#0000FF size=2>**示例--人脸识别：**</font>

```python
# 导入库和模块
from sklearn.datasets import fetch_lfw_people ## 人脸数据
from sklearn.decomposition import PCA 
import matplotlib.pyplot as plt 
import numpy as np 
import pandas as pd 

faces=fetch_lfw_people(min_faces_per_person=60)

faces.images.shape 
#1348*62*47，1348矩阵中图像的个数，62是每个图像的特征矩阵的行，47是每个图像的特征矩阵的列
# 每个图像有62*47=2914个像素
# 每个图像可以可视化
faces.data.shape #2914个特征，1348个图像，无法可视化
faces.data
x=faces.data
## 原特征矩阵可视化
fig,axes=plt.subplots(4,5  ## 多少个图
                     ,figsize=(8,4)  ##行宽的尺寸和比例
                     ,subplot_kw={"xticks":[],"yticks":[]} ## 不要显示坐标轴
                    )

axes[0][0].imshow(faces.images[0,:,:])
fig ##计算机读RGB就是绿色

axes #axes中的一个对象对应fig中的一个空格，二维的
axes.shape
# 两种循环方式，1是使用索引，循环一次同时生成一列上的所有图，2是把数据拉成一维，循环一次只生成一个图
# 根据画的图的信息储存在怎样的结构里，决定用哪种循环方式
# 子图图像.imshow来将图像填充到空白画布上
# imshow的数据格式必须是（m,n）的矩阵，即每个数据都是一张单独的图
# 考虑到我们要遍历的数据faces.images，结构为(1348, 62, 47)
# 因此把axes拉成一维来循环
axes.flat ##二维数组降维成一维
[*axes.flat]  ##惰性对象打开方式
enumerate(axes.flat) # 惰性对象
[*enumerate(axes.flat)] #惰性对象打开方式

for i,ax in enumerate(axes.flat):
    ax.imshow(faces.images[i,:,:]
             ,cmap="gray" #选择色彩的模式
             ) 
# cmp的网址：https://matplotlib.org/tutorials/colors/colormaps.html
fig

## 建模降维，提取新特征空间矩阵
pca=PCA(150).fit(x) #pca只接受二维数据
v=pca.components_ ##新特征向量空间
v.shape

## 新特征空间矩阵可视化
fig,axes=plt.subplots(4,5,figsize=[8,4],subplot_kw={"xticks":[],"yticks":[]})
for i,ax in enumerate(axes.flat):
    ax.imshow(v[i,:].reshape(62,47)
             ,cmap="gray" #选择色彩的模式
             ) 
## 新特征空间中人脸比较模糊
## 整体比较亮的图片，获取信息比较多，眼睛、鼻子、嘴巴相对清晰，脸的轮廓、头发之类的比较模糊
## 整体比较暗的图片，只能看到黑漆漆的一片
## 总结：新特征空间的特征向量们，大部分是”五官“和”亮度“相关的向量

x_dr=pca.transform(x)
x_dr.shape #1348*150
# 将降维后的矩阵inverse_transforma返回原空间
x_inverse=pca.inverse_transform(x_dr)
x_inverse.shape

## 将x和x_inverse可视化
fig,ax=plt.subplots(2,10,figsize=(10,2.5),subplot_kw={"xticks":[],"yticks":[]})

#和2.3.3节中的案例一样，我们需要对子图对象进行遍历的循环，来将图像填入子图中 
#那在这里，我们使用怎样的循环? 
#现在我们的ax中是2行10列，第一行是原数据，第二行是inverse_transform后返回的数据
#所以我们需要同时循环两份数据，即依次循环画一列上的两张图，而不是把ax拉平

for i in range(10):
    ax[0,i].imshow(faces.images[i,:,:],cmap='binary_r')
    ax[1,i].imshow(x_inverse[i].reshape(62,47),cmap='binary_r')
    
## 降维后再通过inverse_transform转换回的数据与原数据画出的大致相似，但原数据的图像更加清晰
## 即inverse_transform并没有实现数据的完全逆转
```

<font color=#0000FF size=2>**示例--用PCA做噪音过滤：**</font>

```python
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA 
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# 导入数据
digits=load_digits()
digits.data.shape

#======================定义画图函数======================
def plot_digits(data):
    fig,axes=plt.subplots(4,10,figsize=(10,4),subplot_kw={"xticks":[],"yticks":[]})
    for i,ax in enumerate(axes.flat):
        ax.imshow(data[i].reshape(8,8),cmap="binary")
plot_digits(digits.data)

#======================给数据加上噪音======================
rng=np.random.RandomState(42)
#在指定的数据集中，随机抽取服从正态分布的数据
#两个参数，分别是指定的数据集和抽取的正态分布的方差
noisy=rng.normal(digits.data,2) ## 从原始数据中抽取一个正态分布，且方差为2（方差越大，信息越离散）
plot_digits(noisy)

noisy.shape

#======================降维======================
pca=PCA(0.5,svd_solver="full").fit(noisy)
x_dr=pca.transform(noisy)
x_dr.shape

pca.explained_variance_ratio_
pca.explained_variance_ratio_.sum()
#======================逆转降维结果，实现降噪======================
without_noise=pca.inverse_transform(x_dr)
plot_digits(without_noise)

### 数字可以看出来，但是比较模糊，噪音被去掉了大部分
```



# 3、案例：PCA对手写数字数据集的降维

**用RFC、KNN示例：**

```python
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt 
import pandas as pd
import numpy as np

#======================导入数据，探索数据======================
data = pd.read_csv(r'/Users/songhaiyue/Documents/课件/03数据预处理和特征工程/digit recognizor【瑞客论坛 www.ruike1.com】.csv')
x=data.iloc[:,1:]
y=data.iloc[:,0]
x.shape

#======================画累计方差贡献曲线，找到降维后维度的范围=====================
pca_line=PCA().fit(x)
plt.figure(figsize=[20,5])
plt.plot(np.cumsum(pca_line.explained_variance_ratio_))
plt.xlabel("number of components after dimension reduction")
plt.ylabel("cumulative explained variance ratio")
plt.show()

#======================降维后维度的学习曲线，继续缩小最佳维度的范围=====================
score=[]
for i in range(1,101,10):
    x_dr=PCA(i).fit_transform(x)
    once=cross_val_score(RFC(n_estimators=10,random_state=0),x_dr,y,cv=5).mean()
    score.append(once)
plt.figure(figsize=[20,5])
plt.plot(range(1,101,10),score)
plt.show()

#======================进一步细化=====================
score=[]
for i in range(10,25):
    x_dr=PCA(i).fit_transform(x)
    once=cross_val_score(RFC(n_estimators=10,random_state=0),x_dr,y,cv=5).mean()
    score.append(once)
plt.figure(figsize=[20,5])
plt.plot(range(10,25),score)
plt.show()

#======================找到最佳维度降维=====================
x_dr=PCA(23).fit_transform(x)
cross_val_score(RFC(n_estimators=100,random_state=0),x_dr,y,cv=5).mean()

## 94.49%效果不如嵌入法特征选择后的96%高

#====================用降维后结果尝试其他模型=======================
from sklearn.neighbors import KNeighborsClassifier as KNN
cross_val_score(KNN(),x_dr,y,cv=5).mean()
# 效果很好，选择最佳K值

#====================KNN的k值学习曲线=======================
score=[]
for i in range(10):
    x_dr=PCA(23).fit_transform(x)
    once=cross_val_score(KNN(i+1),x_dr,y,cv=5).mean()
    score.append(once)
plt.figure(figsize=[20,5])
plt.plot(range(10),score)
plt.show()

# k=4效果最好

#===================k=4效果最好，查看模型运行效果和时间=======================
cross_val_score(KNN(4),x_dr,y,cv=5).mean()

%%timeit 
cross_val_score(KNN(4),x_dr,y,cv=5).mean()
```

![image-20200407101332491](/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200407101332491.png)

<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200407101352360.png" alt="image-20200407101352360" style="zoom:50%;" />

**用SVM进行分析：**可以进一步进行调参，如PCA的n_components

```python
import numpy as np
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt
%matplotlib inline

#向量机
from sklearn.svm import SVC

#主成分分析(principal components analysis),主要用于数据降维的
from sklearn.decomposition import PCA

#用于切割训练数据和样本数据
from sklearn.model_selection import train_test_split

## 生成训练集和测试集
#本地数据
data = pd.read_csv(r'/Users/songhaiyue/Documents/课件/03数据预处理和特征工程/digit recognizor【瑞客论坛 www.ruike1.com】.csv')

train = data.iloc[:,1:]
target = data['label']

#训练数据和样本数据切割
X_train,x_test,y_train,y_true = train_test_split(train,target,test_size=0.2)

## 降维处理 
# 3.1.创建pca对象
pca = PCA(n_components=150,whiten=True)

#3.2.使用pca训练数据
pca.fit(X_train,y_train)

#3.3.对数据进行降维处理
X_train_pca = pca.transform(X_train)
x_test_pca = pca.transform(x_test)

## 创建学习模型
svc = SVC(kernel = 'rbf')
## 模型训练
svc.fit(X_train_pca,y_train)
y_pre_svc = svc.predict(x_test_pca)

# 展示前100的测试样本数据
samples = x_test.iloc[:100]
y_pre = y_pre_svc[:100]
np.arange(16).reshape(2,8)

plt.figure(figsize=(12,18))
for i in range(100):
    plt.subplot(10,10,i+1)
    plt.imshow(np.array(samples.iloc[i]).reshape(28,28),cmap='gray')
    title = 'True:'+str(y_true.iloc[i])+'\nSVC:'+str(y_pre[i])
    plt.title(title)
    plt.axis('off')
    
 # 评估
svc.score(x_test_pca[:100],y_true[:100]) # 98%
```

<img src="/Users/songhaiyue/Library/Application Support/typora-user-images/image-20200407101627458.png" alt="image-20200407101627458" style="zoom:50%;" />

